MLP(
  (layers): Module(
    (0): Module()
    (1): Module()
    (2): Module()
    (3): Module()
    (4): Module()
    (5): Module()
  )
)



def forward(self, x : torch.Tensor):
    layers_0_weight = getattr(self.layers, "0").weight
    pipe_split = colossalai_fx_passes_adding_split_node_pass_pipe_split()
    layers_0_bias = getattr(self.layers, "0").bias
    linear = torch._C._nn.linear(x, layers_0_weight);  x = layers_0_weight = None
    add = linear + layers_0_bias;  linear = layers_0_bias = None
    layers_1_weight = getattr(self.layers, "1").weight
    layers_1_bias = getattr(self.layers, "1").bias
    linear_1 = torch._C._nn.linear(add, layers_1_weight);  add = layers_1_weight = None
    add_1 = linear_1 + layers_1_bias;  linear_1 = layers_1_bias = None
    layers_2_weight = getattr(self.layers, "2").weight
    layers_2_bias = getattr(self.layers, "2").bias
    linear_2 = torch._C._nn.linear(add_1, layers_2_weight);  add_1 = layers_2_weight = None
    add_2 = linear_2 + layers_2_bias;  linear_2 = layers_2_bias = None
    layers_3_weight = getattr(self.layers, "3").weight
    layers_3_bias = getattr(self.layers, "3").bias
    linear_3 = torch._C._nn.linear(add_2, layers_3_weight);  add_2 = layers_3_weight = None
    add_3 = linear_3 + layers_3_bias;  linear_3 = layers_3_bias = None
    layers_4_weight = getattr(self.layers, "4").weight
    layers_4_bias = getattr(self.layers, "4").bias
    linear_4 = torch._C._nn.linear(add_3, layers_4_weight);  add_3 = layers_4_weight = None
    add_4 = linear_4 + layers_4_bias;  linear_4 = layers_4_bias = None
    layers_5_weight = getattr(self.layers, "5").weight
    layers_5_bias = getattr(self.layers, "5").bias
    linear_5 = torch._C._nn.linear(add_4, layers_5_weight);  add_4 = layers_5_weight = None
    add_5 = linear_5 + layers_5_bias;  linear_5 = layers_5_bias = None
    return add_5
    